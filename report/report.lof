\contentsline {figure}{\numberline {1}{\ignorespaces \textbf {Left: Conventional Variational Auto-Encoder}. During generation, a sample z is drawn from a prior $P(z)$ and passed through the feedforward decoder network to compute the probability of the input $P(x|z)$ given the sample. During inference the input x is passed to the encoder network, producing an approximate posterior $Q(z|x)$ over latent variables. During training, z is sampled from $Q(z|x)$ and then used to compute the total loss, which is minimised with stochastic gradient descent. \textbf {Right: DRAW Network}. At each time-step a sample $z_t$ from the prior $P(z_t)$ is passed to the recurrent decoder network, which then modifies part of the canvas matrix. The final canvas matrix $c_T$ is used to compute $P(x|z1:T)$. During inference the input is read at every timestep and the result is passed to the encoder RNN. The RNNs at the previous time-step specify where to read. The output of the encoder RNN is used to compute the approximate posterior over the latent variables at that time-step }}{4}
\contentsline {figure}{\numberline {2}{\ignorespaces Training of DRAW Network. Two losses (Reconstruction and KL Divergence) compete with each other to find an equilibrium.}}{8}
